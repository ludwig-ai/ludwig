



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="Deep learning toolbox">
      
      
        <link rel="canonical" href="https://uber.github.io/ludwig/api/visualization/">
      
      
        <meta name="author" content="Piero Molino">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../favicon.ico">
      <meta name="generator" content="mkdocs-1.1, mkdocs-material-4.6.3">
    
    
      
        <title>Visualization - Ludwig</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/application.adb8469c.css">
      
        <link rel="stylesheet" href="../../assets/stylesheets/application-palette.a8b3c06d.css">
      
      
        
        
        <meta name="theme-color" content="#757575">
      
    
    
      <script src="../../assets/javascripts/modernizr.86422ebf.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../assets/fonts/material-icons.css">
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../../stylesheets/monokai.css">
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="grey" data-md-color-accent="grey">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#module-functions" tabindex="0" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
    <nav class="md-header-nav md-grid">
        <div class="md-flex">
            <div class="md-flex__cell md-flex__cell--shrink">
                <a class="md-header-nav__button md-logo"
                   href="https://uber.github.io/ludwig/" title="Ludwig">
                    <img src="../../images/ludwig_logo.svg" style="height:1.4rem;">
                </a>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
                <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch">
                <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
                    
                    <span class="md-header-nav__topic">
                    Visualization
                    </span>
                    
                </div>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
                
                
                <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
                
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" aria-label="search" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
                
                
            </div>
            
            <div class="md-flex__cell md-flex__cell--shrink">
                <div class="md-header-nav__source">
                    


  

<a href="https://github.com/uber/ludwig/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    uber/ludwig
  </div>
</a>
                </div>
            </div>
            
        </div>
    </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main" role="main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
    <label class="md-nav__title md-nav__title--site" for="__drawer">
        <a class="md-nav__button md-logo" href="https://uber.github.io/ludwig/"
           title="Ludwig">
            <img src="../../images/ludwig_logo.svg" style="width: 10rem">
        </a>
    </label>
    
    <div class="md-nav__source">
        


  

<a href="https://github.com/uber/ludwig/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    uber/ludwig
  </div>
</a>
    </div>
    
    <ul class="md-nav__list" data-md-scrollfix>
        
        
        
        


  <li class="md-nav__item">
    <a href="../.." title="About" class="md-nav__link">
      About
    </a>
  </li>

        
        
        
        


  <li class="md-nav__item">
    <a href="../../getting_started/" title="Getting Started" class="md-nav__link">
      Getting Started
    </a>
  </li>

        
        
        
        


  <li class="md-nav__item">
    <a href="../../examples/" title="Examples" class="md-nav__link">
      Examples
    </a>
  </li>

        
        
        
        


  <li class="md-nav__item">
    <a href="../../user_guide/" title="User Guide" class="md-nav__link">
      User Guide
    </a>
  </li>

        
        
        
        


  <li class="md-nav__item">
    <a href="../../developer_guide/" title="Developer Guide" class="md-nav__link">
      Developer Guide
    </a>
  </li>

        
        
        
        

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6" checked>
    
    <label class="md-nav__link" for="nav-6">
      API
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-6">
        API
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../LudwigModel/" title="LudwigModel" class="md-nav__link">
      LudwigModel
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Visualization
      </label>
    
    <a href="./" title="Visualization" class="md-nav__link md-nav__link--active">
      Visualization
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#module-functions" class="md-nav__link">
    Module functions
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#learning_curves" class="md-nav__link">
    learning_curves
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_performance" class="md-nav__link">
    compare_performance
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_classifiers_performance_from_prob" class="md-nav__link">
    compare_classifiers_performance_from_prob
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_classifiers_performance_from_pred" class="md-nav__link">
    compare_classifiers_performance_from_pred
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_classifiers_performance_subset" class="md-nav__link">
    compare_classifiers_performance_subset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_classifiers_performance_changing_k" class="md-nav__link">
    compare_classifiers_performance_changing_k
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_classifiers_multiclass_multimetric" class="md-nav__link">
    compare_classifiers_multiclass_multimetric
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_classifiers_predictions" class="md-nav__link">
    compare_classifiers_predictions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#confidence_thresholding_2thresholds_2d" class="md-nav__link">
    confidence_thresholding_2thresholds_2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#confidence_thresholding_2thresholds_3d" class="md-nav__link">
    confidence_thresholding_2thresholds_3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#confidence_thresholding" class="md-nav__link">
    confidence_thresholding
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#confidence_thresholding_data_vs_acc" class="md-nav__link">
    confidence_thresholding_data_vs_acc
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#confidence_thresholding_data_vs_acc_subset" class="md-nav__link">
    confidence_thresholding_data_vs_acc_subset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#binary_threshold_vs_metric" class="md-nav__link">
    binary_threshold_vs_metric
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#roc_curves" class="md-nav__link">
    roc_curves
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#roc_curves_from_test_statistics" class="md-nav__link">
    roc_curves_from_test_statistics
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibration_1_vs_all" class="md-nav__link">
    calibration_1_vs_all
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibration_multiclass" class="md-nav__link">
    calibration_multiclass
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#confusion_matrix" class="md-nav__link">
    confusion_matrix
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#frequency_vs_f1" class="md-nav__link">
    frequency_vs_f1
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
      </ul>
    </nav>
  </li>

        
        
        
        


  <li class="md-nav__item">
    <a href="../../faq/" title="FAQ" class="md-nav__link">
      FAQ
    </a>
  </li>

        
    </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#module-functions" class="md-nav__link">
    Module functions
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#learning_curves" class="md-nav__link">
    learning_curves
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_performance" class="md-nav__link">
    compare_performance
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_classifiers_performance_from_prob" class="md-nav__link">
    compare_classifiers_performance_from_prob
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_classifiers_performance_from_pred" class="md-nav__link">
    compare_classifiers_performance_from_pred
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_classifiers_performance_subset" class="md-nav__link">
    compare_classifiers_performance_subset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_classifiers_performance_changing_k" class="md-nav__link">
    compare_classifiers_performance_changing_k
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_classifiers_multiclass_multimetric" class="md-nav__link">
    compare_classifiers_multiclass_multimetric
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare_classifiers_predictions" class="md-nav__link">
    compare_classifiers_predictions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#confidence_thresholding_2thresholds_2d" class="md-nav__link">
    confidence_thresholding_2thresholds_2d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#confidence_thresholding_2thresholds_3d" class="md-nav__link">
    confidence_thresholding_2thresholds_3d
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#confidence_thresholding" class="md-nav__link">
    confidence_thresholding
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#confidence_thresholding_data_vs_acc" class="md-nav__link">
    confidence_thresholding_data_vs_acc
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#confidence_thresholding_data_vs_acc_subset" class="md-nav__link">
    confidence_thresholding_data_vs_acc_subset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#binary_threshold_vs_metric" class="md-nav__link">
    binary_threshold_vs_metric
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#roc_curves" class="md-nav__link">
    roc_curves
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#roc_curves_from_test_statistics" class="md-nav__link">
    roc_curves_from_test_statistics
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibration_1_vs_all" class="md-nav__link">
    calibration_1_vs_all
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibration_multiclass" class="md-nav__link">
    calibration_multiclass
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#confusion_matrix" class="md-nav__link">
    confusion_matrix
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#frequency_vs_f1" class="md-nav__link">
    frequency_vs_f1
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/uber/ludwig/edit/master/mkdocs/docs/api/visualization.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                  <h1>Visualization</h1>
                
                <h2 id="module-functions">Module functions<a class="headerlink" href="#module-functions" title="Permanent link">&para;</a></h2>
<hr />
<h3 id="learning_curves">learning_curves<a class="headerlink" href="#learning_curves" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code><span class="n">ludwig</span><span class="o">.</span><span class="n">visualize</span><span class="o">.</span><span class="n">learning_curves</span><span class="p">(</span>
  <span class="n">train_stats_per_model</span><span class="p">,</span>
  <span class="n">output_feature_name</span><span class="p">,</span>
  <span class="n">model_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">output_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span>
<span class="p">)</span>
</code></pre></div>


<p>Show how model measures change over training and validation data epochs.</p>
<p>For each model and for each output feature and measure of the model,
it produces a line plot showing how that measure changed over the course
of the epochs of training on the training and validation sets.</p>
<p><strong>Inputs</strong></p>
<ul>
<li><strong>train_stats_per_model</strong> (list): List containing train statistics per model</li>
<li><strong>output_feature_name</strong> (string): Name of the output feature that is predicted
   and for which is provided ground truth</li>
<li><strong>model_names</strong> (list, default: None): List of the names of the models to use as labels.</li>
<li><strong>output_directory</strong> (string, default: None): Directory where to save plots.
     If not specified, plots will be displayed in a window</li>
<li><strong>file_format</strong> (string, default: 'pdf'): File format of output plots - pdf or png</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><strong>return</strong> (None):</li>
</ul>
<hr />
<h3 id="compare_performance">compare_performance<a class="headerlink" href="#compare_performance" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code><span class="n">ludwig</span><span class="o">.</span><span class="n">visualize</span><span class="o">.</span><span class="n">compare_performance</span><span class="p">(</span>
  <span class="n">test_stats_per_model</span><span class="p">,</span>
  <span class="n">output_feature_name</span><span class="p">,</span>
  <span class="n">model_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">output_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span>
<span class="p">)</span>
</code></pre></div>


<p>Produces model comparision barplot visualization for each overall metric</p>
<p>For each model (in the aligned lists of test_statistics and model_names)
it produces bars in a bar plot, one for each overall metric available
in the test_statistics file for the specified output_feature_name.</p>
<p><strong>Inputs</strong></p>
<ul>
<li><strong>test_stats_per_model</strong> (list): List containing train statistics per model</li>
<li><strong>output_feature_name</strong> (string): Name of the output feature that is predicted and for which is provided ground truth</li>
<li><strong>model_names</strong> (list, default: None): List of the names of the models to use as labels.</li>
<li><strong>output_directory</strong> (string, default: None): Directory where to save plots.
     If not specified, plots will be displayed in a window</li>
<li><strong>file_format</strong> (string, default: 'pdf'): File format of output plots - pdf or png</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><strong>return</strong> (None):</li>
</ul>
<hr />
<h3 id="compare_classifiers_performance_from_prob">compare_classifiers_performance_from_prob<a class="headerlink" href="#compare_classifiers_performance_from_prob" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code><span class="n">ludwig</span><span class="o">.</span><span class="n">visualize</span><span class="o">.</span><span class="n">compare_classifiers_performance_from_prob</span><span class="p">(</span>
  <span class="n">probabilities_per_model</span><span class="p">,</span>
  <span class="n">ground_truth</span><span class="p">,</span>
  <span class="n">top_n_classes</span><span class="p">,</span>
  <span class="n">labels_limit</span><span class="p">,</span>
  <span class="n">model_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">output_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span>
<span class="p">)</span>
</code></pre></div>


<p>Produces model comparision barplot visualization from probabilities.</p>
<p>For each model it produces bars in a bar plot, one for each overall metric
computed on the fly from the probabilities of predictions for the specified
output_feature_name.</p>
<p><strong>Inputs</strong></p>
<ul>
<li><strong>probabilities_per_model</strong> (list): List of model probabilities</li>
<li><strong>ground_truth</strong> (ndarray): NumPy Array containing ground truth data</li>
<li><strong>top_n_classes</strong> (list): List containing the number of classes to plot</li>
<li><strong>labels_limit</strong> (int): Maximum numbers of labels.
     If labels in dataset are higher than this number, "rare" label</li>
<li><strong>model_names</strong> (list, default: None): List of the names of the models to use as labels.</li>
<li><strong>output_directory</strong> (string, default: None): Directory where to save plots.
     If not specified, plots will be displayed in a window</li>
<li><strong>file_format</strong> (string, default: 'pdf'): File format of output plots - pdf or png</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><strong>return</strong> (None):</li>
</ul>
<hr />
<h3 id="compare_classifiers_performance_from_pred">compare_classifiers_performance_from_pred<a class="headerlink" href="#compare_classifiers_performance_from_pred" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code><span class="n">ludwig</span><span class="o">.</span><span class="n">visualize</span><span class="o">.</span><span class="n">compare_classifiers_performance_from_pred</span><span class="p">(</span>
  <span class="n">predictions_per_model</span><span class="p">,</span>
  <span class="n">ground_truth</span><span class="p">,</span>
  <span class="n">metadata</span><span class="p">,</span>
  <span class="n">output_feature_name</span><span class="p">,</span>
  <span class="n">labels_limit</span><span class="p">,</span>
  <span class="n">model_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">output_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span>
<span class="p">)</span>
</code></pre></div>


<p>Produces model comparision barplot visualization from predictions.</p>
<p>For each model it produces bars in a bar plot, one for each overall metric
computed on the fly from the predictions for the specified output_feature_name.</p>
<p><strong>Inputs</strong></p>
<ul>
<li><strong>predictions_per_model</strong> (list): List containing the model predictions
   for the specified output_feature_name</li>
<li><strong>ground_truth</strong> (ndarray): NumPy Array containing ground truth data</li>
<li><strong>metadata</strong> (dict): Model's input metadata</li>
<li><strong>output_feature_name</strong> (output_feature_name: output_feature_name containing ground trut):output_feature_name: output_feature_name containing ground truth</li>
<li><strong>labels_limit</strong> (labels_limit: Maximum numbers of labels):labels_limit: Maximum numbers of labels.
     If labels in dataset are higher than this number, "rare" label</li>
<li><strong>model_names</strong> (model_names: List of the names of the models to use as labels):model_names: List of the names of the models to use as labels.</li>
<li><strong>output_directory</strong> (output_directory: Directory where to save plots):output_directory: Directory where to save plots.
     If not specified, plots will be displayed in a window</li>
<li><strong>file_format</strong> (file_format: File format of output plots - pdf or p):file_format: File format of output plots - pdf or png</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><strong>return</strong> (None):</li>
</ul>
<hr />
<h3 id="compare_classifiers_performance_subset">compare_classifiers_performance_subset<a class="headerlink" href="#compare_classifiers_performance_subset" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code><span class="n">ludwig</span><span class="o">.</span><span class="n">visualize</span><span class="o">.</span><span class="n">compare_classifiers_performance_subset</span><span class="p">(</span>
  <span class="n">probabilities_per_model</span><span class="p">,</span>
  <span class="n">ground_truth</span><span class="p">,</span>
  <span class="n">top_n_classes</span><span class="p">,</span>
  <span class="n">labels_limit</span><span class="p">,</span>
  <span class="n">subset</span><span class="p">,</span>
  <span class="n">model_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">output_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span>
<span class="p">)</span>
</code></pre></div>


<p>Produces model comparision barplot visualization from train subset.</p>
<p>For each model  it produces bars in a bar plot, one for each overall metric
computed on the fly from the probabilities predictions for the
specified output_feature_name, considering only a subset of the full training set.
The way the subset is obtained is using the top_n_classes and
subset parameters.</p>
<p><strong>Inputs</strong></p>
<ul>
<li><strong>probabilities_per_model</strong> (list): List of model probabilities</li>
<li><strong>ground_truth</strong> (ndarray): NumPy Array containing ground truth data</li>
<li><strong>top_n_classes</strong> (list): List containing the number of classes to plot</li>
<li><strong>labels_limit</strong> (int): Maximum numbers of labels.</li>
<li><strong>subset</strong> (): Type of the subset filtering</li>
<li><strong>model_names</strong> (list, default: None): List of the names of the models to use as labels.</li>
<li><strong>output_directory</strong> (string, default: None): Directory where to save plots.
    If not specified, plots will be displayed in a window</li>
<li><strong>file_format</strong> (string, default: 'pdf'): File format of output plots - pdf or png</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><strong>return</strong> (None):</li>
</ul>
<hr />
<h3 id="compare_classifiers_performance_changing_k">compare_classifiers_performance_changing_k<a class="headerlink" href="#compare_classifiers_performance_changing_k" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code><span class="n">ludwig</span><span class="o">.</span><span class="n">visualize</span><span class="o">.</span><span class="n">compare_classifiers_performance_changing_k</span><span class="p">(</span>
  <span class="n">probabilities_per_model</span><span class="p">,</span>
  <span class="n">ground_truth</span><span class="p">,</span>
  <span class="n">top_k</span><span class="p">,</span>
  <span class="n">labels_limit</span><span class="p">,</span>
  <span class="n">model_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">output_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span>
<span class="p">)</span>
</code></pre></div>


<p>Produce lineplot that show Hits@K measure while k goes from 1 to top_k.</p>
<p>For each model it produces a line plot that shows the Hits@K measure
(that counts a prediction as correct if the model produces it among the
first k) while changing k from 1 to top_k for the specified output_feature_name.</p>
<p><strong>Inputs</strong></p>
<ul>
<li><strong>probabilities_per_model</strong> (list): List of model probabilities</li>
<li><strong>ground_truth</strong> (ndarray): NumPy Array containing ground truth data</li>
<li><strong>top_k</strong> (int): Number of elements in the ranklist to consider</li>
<li><strong>labels_limit</strong> (int): Maximum numbers of labels.
     If labels in dataset are higher than this number, "rare" label</li>
<li><strong>model_names</strong> (list, default: None): List of the names of the models to use as labels.</li>
<li><strong>output_directory</strong> (string, default: None): Directory where to save plots.
     If not specified, plots will be displayed in a window</li>
<li><strong>file_format</strong> (string, default: 'pdf'): File format of output plots - pdf or png</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><strong>return</strong> (None):</li>
</ul>
<hr />
<h3 id="compare_classifiers_multiclass_multimetric">compare_classifiers_multiclass_multimetric<a class="headerlink" href="#compare_classifiers_multiclass_multimetric" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code><span class="n">ludwig</span><span class="o">.</span><span class="n">visualize</span><span class="o">.</span><span class="n">compare_classifiers_multiclass_multimetric</span><span class="p">(</span>
  <span class="n">test_stats_per_model</span><span class="p">,</span>
  <span class="n">metadata</span><span class="p">,</span>
  <span class="n">output_feature_name</span><span class="p">,</span>
  <span class="n">top_n_classes</span><span class="p">,</span>
  <span class="n">model_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">output_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span>
<span class="p">)</span>
</code></pre></div>


<p>Show the precision, recall and F1 of the model for the specified output_feature_name.</p>
<p>For each model it produces four plots that show the precision,
recall and F1 of the model on several classes for the specified output_feature_name.</p>
<p><strong>Inputs</strong></p>
<ul>
<li><strong>test_stats_per_model</strong> (list): List containing train statistics per model</li>
<li><strong>metadata</strong> (dict): Model's input metadata</li>
<li><strong>output_feature_name</strong> (string): Name of the output feature that is predicted and for which is provided ground truth</li>
<li><strong>top_n_classes</strong> (list): List containing the number of classes to plot</li>
<li><strong>model_names</strong> (list, default: None): List of the names of the models to use as labels.</li>
<li><strong>output_directory</strong> (string, default: None): Directory where to save plots.
     If not specified, plots will be displayed in a window</li>
<li><strong>file_format</strong> (string, default: 'pdf'): File format of output plots - pdf or png</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><strong>return</strong> (None):</li>
</ul>
<hr />
<h3 id="compare_classifiers_predictions">compare_classifiers_predictions<a class="headerlink" href="#compare_classifiers_predictions" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code><span class="n">ludwig</span><span class="o">.</span><span class="n">visualize</span><span class="o">.</span><span class="n">compare_classifiers_predictions</span><span class="p">(</span>
  <span class="n">predictions_per_model</span><span class="p">,</span>
  <span class="n">ground_truth</span><span class="p">,</span>
  <span class="n">labels_limit</span><span class="p">,</span>
  <span class="n">model_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">output_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span>
<span class="p">)</span>
</code></pre></div>


<p>Show two models comparision of their output_feature_name predictions.</p>
<p><strong>Inputs</strong></p>
<ul>
<li><strong>predictions_per_model</strong> (list): List containing the model predictions</li>
<li><strong>ground_truth</strong> (ndarray): NumPy Array containing ground truth data</li>
<li><strong>labels_limit</strong> (int): Maximum numbers of labels.
     If labels in dataset are higher than this number, "rare" label</li>
<li><strong>model_names</strong> (list, default: None): List of the names of the models to use as labels.</li>
<li><strong>output_directory</strong> (string, default: None): Directory where to save plots.
     If not specified, plots will be displayed in a window</li>
<li><strong>file_format</strong> (string, default: 'pdf'): File format of output plots - pdf or png</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><strong>return</strong> (None):</li>
</ul>
<hr />
<h3 id="confidence_thresholding_2thresholds_2d">confidence_thresholding_2thresholds_2d<a class="headerlink" href="#confidence_thresholding_2thresholds_2d" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code><span class="n">ludwig</span><span class="o">.</span><span class="n">visualize</span><span class="o">.</span><span class="n">confidence_thresholding_2thresholds_2d</span><span class="p">(</span>
  <span class="n">probabilities_per_model</span><span class="p">,</span>
  <span class="n">ground_truths</span><span class="p">,</span>
  <span class="n">threshold_output_feature_names</span><span class="p">,</span>
  <span class="n">labels_limit</span><span class="p">,</span>
  <span class="n">model_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">output_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span>
<span class="p">)</span>
</code></pre></div>


<p>Show confidence trethreshold data vs accuracy for two output_feature_name thresholds</p>
<p>The first plot shows several semi transparent lines. They summarize the
3d surfaces displayed by confidence_thresholding_2thresholds_3d that have
thresholds on the confidence of the predictions of the two
threshold_output_feature_names  as x and y axes and either the data coverage percentage or
the accuracy as z axis. Each line represents a slice of the data
coverage  surface projected onto the accuracy surface.</p>
<p><strong>Inputs</strong></p>
<ul>
<li><strong>probabilities_per_model</strong> (list): List of model probabilities</li>
<li><strong>ground_truths</strong> (list): List of NumPy Arrays containing ground truth data</li>
<li><strong>threshold_output_feature_names</strong> (list): List of output_feature_names for 2d threshold</li>
<li><strong>labels_limit</strong> (int): Maximum numbers of labels.
     If labels in dataset are higher than this number, "rare" label</li>
<li><strong>model_names</strong> (string): Name of the model to use as label.</li>
<li><strong>output_directory</strong> (string, default: None): Directory where to save plots.
     If not specified, plots will be displayed in a window</li>
<li><strong>file_format</strong> (string, default: 'pdf'): File format of output plots - pdf or png</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><strong>return</strong> (None):</li>
</ul>
<hr />
<h3 id="confidence_thresholding_2thresholds_3d">confidence_thresholding_2thresholds_3d<a class="headerlink" href="#confidence_thresholding_2thresholds_3d" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code><span class="n">ludwig</span><span class="o">.</span><span class="n">visualize</span><span class="o">.</span><span class="n">confidence_thresholding_2thresholds_3d</span><span class="p">(</span>
  <span class="n">probabilities_per_model</span><span class="p">,</span>
  <span class="n">ground_truths</span><span class="p">,</span>
  <span class="n">threshold_output_feature_names</span><span class="p">,</span>
  <span class="n">labels_limit</span><span class="p">,</span>
  <span class="n">output_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span>
<span class="p">)</span>
</code></pre></div>


<p>Show 3d confidence trethreshold data vs accuracy for two output_feature_name thresholds</p>
<p>The plot shows the 3d surfaces displayed by
confidence_thresholding_2thresholds_3d that have thresholds on the
confidence of the predictions of the two threshold_output_feature_names as x and y axes
and either the data coverage percentage or the accuracy as z axis.</p>
<p><strong>Inputs</strong></p>
<ul>
<li><strong>probabilities_per_model</strong> (list): List of model probabilities</li>
<li><strong>ground_truths</strong> (list): List of NumPy Arrays containing ground truth data</li>
<li><strong>threshold_output_feature_names</strong> (list): List of output_feature_names for 2d threshold</li>
<li><strong>labels_limit</strong> (int): Maximum numbers of labels.
     If labels in dataset are higher than this number, "rare" label</li>
<li><strong>output_directory</strong> (string, default: None): Directory where to save plots.
     If not specified, plots will be displayed in a window</li>
<li><strong>file_format</strong> (string, default: 'pdf'): File format of output plots - pdf or png</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><strong>return</strong> (None):</li>
</ul>
<hr />
<h3 id="confidence_thresholding">confidence_thresholding<a class="headerlink" href="#confidence_thresholding" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code><span class="n">ludwig</span><span class="o">.</span><span class="n">visualize</span><span class="o">.</span><span class="n">confidence_thresholding</span><span class="p">(</span>
  <span class="n">probabilities_per_model</span><span class="p">,</span>
  <span class="n">ground_truth</span><span class="p">,</span>
  <span class="n">labels_limit</span><span class="p">,</span>
  <span class="n">model_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">output_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span>
<span class="p">)</span>
</code></pre></div>


<p>Show models accuracy and data coverage while increasing treshold</p>
<p>For each model it produces a pair of lines indicating the accuracy of
the model and the data coverage while increasing a threshold (x axis) on
the probabilities of predictions for the specified output_feature_name.</p>
<p><strong>Inputs</strong></p>
<ul>
<li><strong>probabilities_per_model</strong> (list): List of model probabilities</li>
<li><strong>ground_truth</strong> (ndarray): NumPy Array containing ground truth data</li>
<li><strong>labels_limit</strong> (int): Maximum numbers of labels.
     If labels in dataset are higher than this number, "rare" label</li>
<li><strong>model_names</strong> (list, default: None): List of the names of the models to use as labels.</li>
<li><strong>output_directory</strong> (sting): Directory where to save plots.
     If not specified, plots will be displayed in a window</li>
<li><strong>file_format</strong> (string, default: 'pdf'): File format of output plots - pdf or png</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><strong>return</strong> (None):</li>
</ul>
<hr />
<h3 id="confidence_thresholding_data_vs_acc">confidence_thresholding_data_vs_acc<a class="headerlink" href="#confidence_thresholding_data_vs_acc" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code><span class="n">ludwig</span><span class="o">.</span><span class="n">visualize</span><span class="o">.</span><span class="n">confidence_thresholding_data_vs_acc</span><span class="p">(</span>
  <span class="n">probabilities_per_model</span><span class="p">,</span>
  <span class="n">ground_truth</span><span class="p">,</span>
  <span class="n">labels_limit</span><span class="p">,</span>
  <span class="n">model_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">output_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span>
<span class="p">)</span>
</code></pre></div>


<p>Show models comparision of confidence treshold data vs accuracy.</p>
<p>For each model it produces a line indicating the accuracy of the model
and the data coverage while increasing a threshold on the probabilities
of predictions for the specified output_feature_name. The difference with
confidence_thresholding is that it uses two axes instead of three,
not visualizing the threshold and having coverage as x axis instead of
the threshold.</p>
<p><strong>Inputs</strong></p>
<ul>
<li><strong>probabilities_per_model</strong> (list): List of model probabilities</li>
<li><strong>ground_truth</strong> (ndarray): NumPy Array containing ground truth data</li>
<li><strong>labels_limit</strong> (int): Maximum numbers of labels.
     If labels in dataset are higher than this number, "rare" label</li>
<li><strong>model_names</strong> (list, default: None): List of the names of the models to use as labels.</li>
<li><strong>output_directory</strong> (string, default: None): Directory where to save plots.
     If not specified, plots will be displayed in a window</li>
<li><strong>file_format</strong> (string, default: 'pdf'): File format of output plots - pdf or png</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><strong>return</strong> (None):</li>
</ul>
<hr />
<h3 id="confidence_thresholding_data_vs_acc_subset">confidence_thresholding_data_vs_acc_subset<a class="headerlink" href="#confidence_thresholding_data_vs_acc_subset" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code><span class="n">ludwig</span><span class="o">.</span><span class="n">visualize</span><span class="o">.</span><span class="n">confidence_thresholding_data_vs_acc_subset</span><span class="p">(</span>
  <span class="n">probabilities_per_model</span><span class="p">,</span>
  <span class="n">ground_truth</span><span class="p">,</span>
  <span class="n">top_n_classes</span><span class="p">,</span>
  <span class="n">labels_limit</span><span class="p">,</span>
  <span class="n">subset</span><span class="p">,</span>
  <span class="n">model_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">output_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span>
<span class="p">)</span>
</code></pre></div>


<p>Show models comparision of confidence treshold data vs accuracy on a
subset of data.</p>
<p>For each model it produces a line indicating the accuracy of the model
and the data coverage while increasing a threshold on the probabilities
of predictions for the specified output_feature_name, considering only a subset of the
full training set. The way the subset is obtained is using the top_n_classes
and subset parameters.
The difference with confidence_thresholding is that it uses two axes
instead of three, not visualizing the threshold and having coverage as
x axis instead of the threshold.</p>
<p>If the values of subset is ground_truth, then only datapoints where the
ground truth class is within the top n most frequent ones will be
considered  as test set, and the percentage of datapoints that have been
kept  from the original set will be displayed. If the values of subset is
predictions, then only datapoints where the the model predicts a class
that is within the top n most frequent ones will be considered as test set,
and the percentage of datapoints that have been kept from the original set
will be displayed for each model.</p>
<p><strong>Inputs</strong></p>
<ul>
<li><strong>probabilities_per_model</strong> (list): List of model probabilities</li>
<li><strong>ground_truth</strong> (ndarray): NumPy Array containing ground truth data</li>
<li><strong>top_n_classes</strong> (list): List containing the number of classes to plot</li>
<li><strong>labels_limit</strong> (int): Maximum numbers of labels.</li>
<li><strong>subset</strong> (string): Type of the subset filtering</li>
<li><strong>model_names</strong> (list, default: None): List of the names of the models to use as labels.</li>
<li><strong>output_directory</strong> (string, default: None): Directory where to save plots.
     If not specified, plots will be displayed in a window</li>
<li><strong>file_format</strong> (string, default: 'pdf'): File format of output plots - pdf or png</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><strong>return</strong> (None):</li>
</ul>
<hr />
<h3 id="binary_threshold_vs_metric">binary_threshold_vs_metric<a class="headerlink" href="#binary_threshold_vs_metric" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code><span class="n">ludwig</span><span class="o">.</span><span class="n">visualize</span><span class="o">.</span><span class="n">binary_threshold_vs_metric</span><span class="p">(</span>
  <span class="n">probabilities_per_model</span><span class="p">,</span>
  <span class="n">ground_truth</span><span class="p">,</span>
  <span class="n">metrics</span><span class="p">,</span>
  <span class="n">positive_label</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
  <span class="n">model_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">output_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span>
<span class="p">)</span>
</code></pre></div>


<p>Show confidence of the model against metric for the specified output_feature_name.</p>
<p>For each metric specified in metrics (options are f1, precision, recall,
accuracy), this visualization produces a line chart plotting a threshold
on  the confidence of the model against the metric for the specified
output_feature_name.  If output_feature_name is a category feature, positive_label indicates which is
the class to be considered positive class and all the others will be
considered negative. It needs to be an integer, to figure out the
association between classes and integers check the ground_truth_metadata
JSON file.</p>
<p><strong>Inputs</strong></p>
<ul>
<li><strong>probabilities_per_model</strong> (list): List of model probabilities</li>
<li><strong>ground_truth</strong> (list): List of NumPy Arrays containing ground truth data</li>
<li><strong>metrics</strong> (f1, precision, recall):metrics: metrics to dispay (f1, precision, recall,
            accuracy)</li>
<li><strong>positive_label</strong> (string): Label of the positive class</li>
<li><strong>model_names</strong> (list, default: None): List of the names of the models to use as labels.</li>
<li><strong>output_directory</strong> (string, default: None): Directory where to save plots.
     If not specified, plots will be displayed in a window</li>
<li><strong>file_format</strong> (string, default: 'pdf'): File format of output plots - pdf or png</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><strong>return</strong> (None):</li>
</ul>
<hr />
<h3 id="roc_curves">roc_curves<a class="headerlink" href="#roc_curves" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code><span class="n">ludwig</span><span class="o">.</span><span class="n">visualize</span><span class="o">.</span><span class="n">roc_curves</span><span class="p">(</span>
  <span class="n">probabilities_per_model</span><span class="p">,</span>
  <span class="n">ground_truth</span><span class="p">,</span>
  <span class="n">positive_label</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
  <span class="n">model_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">output_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span>
<span class="p">)</span>
</code></pre></div>


<p>Show the roc curves for the specified models output output_feature_name.</p>
<p>This visualization produces a line chart plotting the roc curves for the
specified output_feature_name. If output_feature_name is a category feature, positive_label indicates
which is the class to be considered positive class and all the others will
be considered negative. It needs to be an integer, to figure out the
association between classes and integers check the ground_truth_metadata
JSON file.</p>
<p><strong>Inputs</strong></p>
<ul>
<li><strong>probabilities_per_model</strong> (list): List of model probabilities</li>
<li><strong>ground_truth</strong> (list): List of NumPy Arrays containing ground truth data</li>
<li><strong>positive_label</strong> (string): Label of the positive class</li>
<li><strong>model_names</strong> (list, default: None): List of the names of the models to use as labels.</li>
<li><strong>output_directory</strong> (string, default: None): Directory where to save plots.
     If not specified, plots will be displayed in a window</li>
<li><strong>file_format</strong> (string, default: 'pdf'): File format of output plots - pdf or png</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><strong>return</strong> (None):</li>
</ul>
<hr />
<h3 id="roc_curves_from_test_statistics">roc_curves_from_test_statistics<a class="headerlink" href="#roc_curves_from_test_statistics" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code><span class="n">ludwig</span><span class="o">.</span><span class="n">visualize</span><span class="o">.</span><span class="n">roc_curves_from_test_statistics</span><span class="p">(</span>
  <span class="n">test_stats_per_model</span><span class="p">,</span>
  <span class="n">output_feature_name</span><span class="p">,</span>
  <span class="n">model_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">output_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span>
<span class="p">)</span>
</code></pre></div>


<p>Show the roc curves for the specified models output binary output_feature_name.</p>
<p>This visualization uses the output_feature_name, test_statistics and model_names
parameters. output_feature_name needs to be binary feature. This visualization produces a
line chart plotting the roc curves for the specified output_feature_name.</p>
<p><strong>Inputs</strong></p>
<ul>
<li><strong>test_stats_per_model</strong> (list): List containing train statistics per model</li>
<li><strong>output_feature_name</strong> (string): Name of the output feature that is predicted and for which is provided ground truth</li>
<li><strong>model_names</strong> (list, default: None): List of the names of the models to use as labels.</li>
<li><strong>output_directory</strong> (string, default: None): Directory where to save plots.
     If not specified, plots will be displayed in a window</li>
<li><strong>file_format</strong> (string, default: 'pdf'): File format of output plots - pdf or png</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><strong>return</strong> (None):</li>
</ul>
<hr />
<h3 id="calibration_1_vs_all">calibration_1_vs_all<a class="headerlink" href="#calibration_1_vs_all" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code><span class="n">ludwig</span><span class="o">.</span><span class="n">visualize</span><span class="o">.</span><span class="n">calibration_1_vs_all</span><span class="p">(</span>
  <span class="n">probabilities_per_model</span><span class="p">,</span>
  <span class="n">ground_truth</span><span class="p">,</span>
  <span class="n">top_n_classes</span><span class="p">,</span>
  <span class="n">labels_limit</span><span class="p">,</span>
  <span class="n">model_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">output_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span>
<span class="p">)</span>
</code></pre></div>


<p>Show models probability of predictions for the specified output_feature_name.</p>
<p>For each class or each of the k most frequent classes if top_k is
specified,  it produces two plots computed on the fly from the
probabilities  of predictions for the specified output_feature_name.</p>
<p>The first plot is a calibration curve that shows the calibration of the
predictions considering the current class to be the true one and all
others  to be a false one, drawing one line for each model (in the
aligned  lists of probabilities and model_names).</p>
<p>The second plot shows the distributions of the predictions considering
the  current class to be the true one and all others to be a false one,
drawing the distribution for each model (in the aligned lists of
probabilities and model_names).</p>
<p><strong>Inputs</strong></p>
<ul>
<li><strong>probabilities_per_model</strong> (list): List of model probabilities</li>
<li><strong>ground_truth</strong> (ndarray): NumPy Array containing ground truth data</li>
<li><strong>top_n_classes</strong> (list): List containing the number of classes to plot</li>
<li><strong>labels_limit</strong> (int): Maximum numbers of labels.
     If labels in dataset are higher than this number, "rare" label</li>
<li><strong>model_names</strong> (list, default: None): List of the names of the models to use as labels.</li>
<li><strong>output_directory</strong> (string, default: None): Directory where to save plots.
     If not specified, plots will be displayed in a window</li>
<li><strong>file_format</strong> (string, default: 'pdf'): File format of output plots - pdf or png</li>
</ul>
<p><strong>String</strong></p>
<ul>
<li><strong>return</strong> (None):</li>
</ul>
<hr />
<h3 id="calibration_multiclass">calibration_multiclass<a class="headerlink" href="#calibration_multiclass" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code><span class="n">ludwig</span><span class="o">.</span><span class="n">visualize</span><span class="o">.</span><span class="n">calibration_multiclass</span><span class="p">(</span>
  <span class="n">probabilities_per_model</span><span class="p">,</span>
  <span class="n">ground_truth</span><span class="p">,</span>
  <span class="n">labels_limit</span><span class="p">,</span>
  <span class="n">model_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">output_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span>
<span class="p">)</span>
</code></pre></div>


<p>Show models probability of predictions for each class of the the
specified output_feature_name.</p>
<p><strong>Inputs</strong></p>
<ul>
<li><strong>probabilities_per_model</strong> (list): List of model probabilities</li>
<li><strong>ground_truth</strong> (ndarray): NumPy Array containing ground truth data</li>
<li><strong>labels_limit</strong> (int): Maximum numbers of labels.
     If labels in dataset are higher than this number, "rare" label</li>
<li><strong>model_names</strong> (list, default: None): List of the names of the models to use as labels.</li>
<li><strong>output_directory</strong> (string, default: None): Directory where to save plots.
     If not specified, plots will be displayed in a window</li>
<li><strong>file_format</strong> (string, default: 'pdf'): File format of output plots - pdf or png</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><strong>return</strong> (None):</li>
</ul>
<hr />
<h3 id="confusion_matrix">confusion_matrix<a class="headerlink" href="#confusion_matrix" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code><span class="n">ludwig</span><span class="o">.</span><span class="n">visualize</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span>
  <span class="n">test_stats_per_model</span><span class="p">,</span>
  <span class="n">metadata</span><span class="p">,</span>
  <span class="n">output_feature_name</span><span class="p">,</span>
  <span class="n">top_n_classes</span><span class="p">,</span>
  <span class="n">normalize</span><span class="p">,</span>
  <span class="n">model_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">output_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span>
<span class="p">)</span>
</code></pre></div>


<p>Show confision matrix in the models predictions for each output_feature_name.</p>
<p>For each model (in the aligned lists of test_statistics and model_names)
it  produces a heatmap of the confusion matrix in the predictions for
each  output_feature_name that has a confusion matrix in test_statistics. The value of
top_n_classes limits the heatmap to the n most frequent classes.</p>
<p><strong>Inputs</strong></p>
<ul>
<li><strong>test_stats_per_model</strong> (string): List containing train statistics per model</li>
<li><strong>metadata</strong> (dict): Model's input metadata</li>
<li><strong>output_feature_name</strong> (string): Name of the output feature that is predicted and for which is provided ground truth</li>
<li><strong>top_n_classes</strong> (list): List containing the number of classes to plot</li>
<li><strong>normalize</strong> (bool): Flag to normalize rows in confusion matrix</li>
<li><strong>model_names</strong> (list, default: None): List of the names of the models to use as labels.</li>
<li><strong>output_directory</strong> (string, default: None): Directory where to save plots.
     If not specified, plots will be displayed in a window</li>
<li><strong>file_format</strong> (string, default: 'pdf'): File format of output plots - pdf or png</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><strong>return</strong> (None):</li>
</ul>
<hr />
<h3 id="frequency_vs_f1">frequency_vs_f1<a class="headerlink" href="#frequency_vs_f1" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code><span class="n">ludwig</span><span class="o">.</span><span class="n">visualize</span><span class="o">.</span><span class="n">frequency_vs_f1</span><span class="p">(</span>
  <span class="n">test_stats_per_model</span><span class="p">,</span>
  <span class="n">metadata</span><span class="p">,</span>
  <span class="n">output_feature_name</span><span class="p">,</span>
  <span class="n">top_n_classes</span><span class="p">,</span>
  <span class="n">model_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">output_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">file_format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span>
<span class="p">)</span>
</code></pre></div>


<p>Show prediction statistics for the specified output_feature_name for each model.</p>
<p>For each model (in the aligned lists of test_statistics and model_names),
produces two plots statistics of predictions for the specified output_feature_name.</p>
<p>The first plot is a line plot with one x axis representing the different
classes and two vertical axes colored in orange and blue respectively.
The orange one is the frequency of the class and an orange line is plotted
to show the trend. The blue one is the F1 score for that class and a blue
line is plotted to show the trend. The classes on the x axis are sorted by
f1 score.
The second plot has the same structure of the first one,
but the axes are flipped and the classes on the x axis are sorted by
frequency.</p>
<p><strong>Inputs</strong></p>
<ul>
<li><strong>test_stats_per_model</strong> (list): List containing train statistics per model</li>
<li><strong>metadata</strong> (dict): Model's input metadata</li>
<li><strong>output_feature_name</strong> (string): Name of the output feature that is predicted and for which is provided ground truth</li>
<li><strong>top_n_classes</strong> (list): List containing the number of classes to plot</li>
<li><strong>model_names</strong> (list, default: None): List of the names of the models to use as labels.</li>
<li><strong>output_directory</strong> (string, default: None): Directory where to save plots.
     If not specified, plots will be displayed in a window</li>
<li><strong>file_format</strong> (string, default: 'pdf'): File format of output plots - pdf or png</li>
</ul>
<p><strong>Return</strong></p>
<ul>
<li><strong>return</strong> (None):</li>
</ul>
                
                  
                
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        

<!-- Application footer -->
<footer class="md-footer">

    <!-- Link to previous and/or next page -->
    
    <div class="md-footer-nav">
        <nav class="md-footer-nav__inner md-grid">

            <!-- Link to previous page -->
            
            <a class="md-flex md-footer-nav__link md-footer-nav__link--prev"
               href="../LudwigModel/"
               rel="prev"
               title="LudwigModel">
                <div class="md-flex__cell md-flex__cell--shrink">
                    <i class="md-icon md-icon--arrow-back
                    md-footer-nav__button"></i>
                </div>
                <div class="md-flex__cell md-flex__cell--stretch
                  md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                LudwigModel
              </span>
                </div>
            </a>
            

            <!-- Link to next page -->
            
            <a class="md-flex md-footer-nav__link md-footer-nav__link--next"
               href="../../faq/"
               rel="next"
               title="FAQ">
                <div class="md-flex__cell md-flex__cell--stretch
                  md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                FAQ
              </span>
                </div>
                <div class="md-flex__cell md-flex__cell--shrink">
                    <i class="md-icon md-icon--arrow-forward
                    md-footer-nav__button"></i>
                </div>
            </a>
            
        </nav>
    </div>
    

    <!-- Further information -->
    <div class="md-footer-meta md-typeset">
        <div class="md-footer-meta__inner md-grid">

            <!-- Copyright and theme information -->
            <div class="md-footer-copyright">
                <div class="footer-logo-smallpad"></div>
                
                <div class="md-footer-copyright__highlight">
                    Copyright &copy; 2018 - 2019 Uber Technologies Inc.
                </div>
                
                Website by <a href="http://w4nderlu.st">w4nderlust</a> powered by
                <a href="https://www.mkdocs.org">MkDocs</a>,
                <a href="https://squidfunk.github.io/mkdocs-material/">Material for MkDocs</a>,
                <a href="http://www.styleshout.com/">styleshout</a> and
                <a href="http://cables.gl/">cables</a>.
            </div>

            <!-- Social links -->
            
            
            
        </div>
    </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/application.c33a9706.js"></script>
      
      <script>app.initialize({version:"1.1",url:{base:"../.."}})</script>
      
    
  </body>
</html>